<!DOCTYPE html><html lang="th"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><title>Module 1: Beyond Prompting - AI Architecture</title><link rel="stylesheet" href="shared/style.css"><script src="shared/scorm.js"></script></head><body><div class="container"><div class="header"><h1>ü§ñ Module 1</h1><p class="subtitle">Beyond Prompting - AI Architecture</p></div><div class="content"><div class="section"><h2>üìö Transformer Architecture & Attention Mechanism</h2><p>‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI Models ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô</p><div class="objectives"><h3>üéØ Learning Objectives</h3><ul><li>‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI Models</li><li>‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Models</li><li>‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô AI</li></ul></div><h3>Transformer Architecture</h3><p>Transformer ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏è‡∏¥‡∏ß‡∏±‡∏ï‡∏¥‡∏ß‡∏á‡∏Å‡∏≤‡∏£ AI ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Attention Mechanism ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö</p><div class="tip-box"><strong>üí° Key Concept:</strong> Self-Attention ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô</div></div><div class="section"><h2>üîß LLM Training Process</h2><h3>1. Pre-training</h3><p>‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (Billions of tokens)</p><h3>2. Fine-tuning</h3><p>‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞</p><h3>3. RLHF (Reinforcement Learning from Human Feedback)</h3><p>‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ feedback ‡∏à‡∏≤‡∏Å‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå</p></div><div class="section"><h2>üí∞ Token Economics & Cost Optimization</h2><div class="example-box"><h3>üí° Practical Example: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Models</h3><ul><li><strong>GPT-4:</strong> $0.03/1K tokens (input), $0.06/1K tokens (output)</li><li><strong>Claude 3:</strong> $0.015/1K tokens (input), $0.075/1K tokens (output)</li><li><strong>Llama 2:</strong> Open source - ‡∏ü‡∏£‡∏µ (‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ infrastructure)</li></ul></div><div class="tip-box"><strong>üí° Cost Optimization Tips:</strong><ul><li>‡πÉ‡∏ä‡πâ caching ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompts ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥</li><li>‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô</li><li>‡∏•‡∏î token count ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ compress prompts</li></ul></div></div></div><div class="nav-buttons"><button class="btn btn-secondary" onclick="window.history.back()">‚Üê ‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö</button><button class="btn btn-primary" onclick="window.location.href='module2.html';setComplete()">‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‚Üí</button></div></div></body></html>
