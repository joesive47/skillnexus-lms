import { PrismaClient } from '@prisma/client'
import { readFileSync } from 'fs'
import { pipeline } from '@xenova/transformers'

const prisma = new PrismaClient()

// à¸ªà¸£à¹‰à¸²à¸‡ embedding
let embedder = null

async function getEmbedder() {
  if (!embedder) {
    console.log('Loading embedding model...')
    embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2')
  }
  return embedder
}

async function generateEmbedding(text) {
  try {
    const model = await getEmbedder()
    const output = await model(text, { pooling: 'mean', normalize: true })
    return Array.from(output.data)
  } catch (error) {
    console.error('Embedding generation error:', error)
    return []
  }
}

async function importKnowledgeBase() {
  try {
    console.log('ğŸš€ Starting enhanced knowledge base import...')
    
    // à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ JSON
    const knowledgeData = JSON.parse(
      readFileSync('public/knowledge-base-1763982823686.json', 'utf8')
    )
    
    console.log(`ğŸ“š Found ${knowledgeData.knowledge.length} knowledge chunks`)
    
    // à¸ªà¸£à¹‰à¸²à¸‡ document record à¸à¹ˆà¸­à¸™
    for (const doc of knowledgeData.metadata.documents) {
      await prisma.document.upsert({
        where: { id: doc.id },
        update: {
          filename: doc.filename,
          fileType: doc.fileType || 'json',
          uploadedAt: new Date()
        },
        create: {
          id: doc.id,
          filename: doc.filename,
          fileType: doc.fileType || 'json',
          uploadedAt: new Date()
        }
      })
      console.log(`ğŸ“„ Document: ${doc.filename}`)
    }
    
    // Import knowledge chunks à¸à¸£à¹‰à¸­à¸¡ embedding
    let processed = 0
    for (const item of knowledgeData.knowledge) {
      console.log(`Processing ${++processed}/${knowledgeData.knowledge.length}: ${item.documentName}`)
      
      // à¸ªà¸£à¹‰à¸²à¸‡ embedding
      const embedding = await generateEmbedding(item.content)
      
      // à¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
      await prisma.documentChunk.upsert({
        where: { id: item.id },
        update: {
          content: item.content,
          embedding: JSON.stringify(embedding),
          metadata: item.metadata || {}
        },
        create: {
          id: item.id,
          documentId: item.documentId,
          content: item.content,
          embedding: JSON.stringify(embedding),
          metadata: item.metadata || {}
        }
      })
    }
    
    console.log('âœ… Enhanced knowledge base import completed!')
    
    // à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š
    console.log('\nğŸ§ª Testing search functionality...')
    
    const testQueries = [
      'SkillNexus LMS à¸¡à¸µà¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡',
      'à¸£à¸°à¸šà¸š Anti-Skip à¸„à¸·à¸­à¸­à¸°à¹„à¸£',
      'à¸£à¸­à¸‡à¸£à¸±à¸š SCORM à¹„à¸«à¸¡',
      'PWA à¸„à¸·à¸­à¸­à¸°à¹„à¸£',
      'à¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£'
    ]
    
    for (const query of testQueries) {
      const chunks = await prisma.documentChunk.findMany({
        where: { embedding: { not: null } },
        include: { document: true }
      })
      
      console.log(`Query: "${query}" -> Found ${chunks.length} chunks`)
    }
    
    console.log('\nğŸ‰ All done! Chatbot is ready to use.')
    
  } catch (error) {
    console.error('âŒ Import failed:', error)
  } finally {
    await prisma.$disconnect()
  }
}

importKnowledgeBase()